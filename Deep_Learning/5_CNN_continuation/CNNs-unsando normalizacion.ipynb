{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233638a8",
   "metadata": {},
   "source": [
    "# CNN - redes neuronales convolucionales\n",
    "\n",
    "Vamos a implementar diferentes modelos de redes convolucionales y realizar el ejercicio de probar su funcionamiento con la base de datos MNIST\n",
    "\n",
    "### Caso 1:\n",
    "\n",
    "Ejemplo de una red neuronal convolucional (CNN) de tres capas que utiliza TensorFlow para clasificar imágenes. Este ejemplo supone una arquitectura simple con dos capas convolucionales seguidas de agrupación máxima y una capa completamente conectada para la clasificación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18495160",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define el modelo CNN\n",
    "model = models.Sequential()\n",
    "\n",
    "# Primera capa convolucional\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Segunda capa convolucional\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten Layer : permite concatenar las caracteristicas en un vector unidimensional (aplanar)\n",
    "#sirve para preparar los datos de entrada a una red completamente conectada (FC - fully connected)\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully Connected Layer\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Capa de salida\n",
    "model.add(layers.Dense(10, activation='softmax'))  # En este caso se tienen 10 clases \n",
    "\n",
    "# Compile el modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# mostar el resumen\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "**Explicación:**\n",
    "\n",
    "1. **Capas convolucionales**: las dos primeras capas son capas convolucionales. Están diseñados para aprender características a partir de los datos de entrada. Las capas `Conv2D` aplican un conjunto de filtros a la entrada y cada filtro aprende una característica diferente.\n",
    "\n",
    "2. **Capas pooling o agrupacion MaxPooling**: después de cada capa convolucional, hay una capa de agrupación  (`MaxPooling2D`). Esta capa reduce las dimensiones espaciales (ancho y alto) de los datos, lo que ayuda a reducir el cálculo y evita el sobreajuste.\n",
    "\n",
    "3. **Flatten Layer**: esta capa se utiliza para convertir los datos multidimensionales en una matriz unidimensional. Esto es necesario antes de pasar los datos a capas completamente conectadas.\n",
    "\n",
    "4. **Capas completamente conectadas - Fully connected**: los datos aplanados pasan a través de una o más capas completamente conectadas, que realizan la tarea de clasificación.\n",
    "\n",
    "5. **Capa de salida**: la capa final utiliza la función de activación softmax para generar probabilidades para cada clase.\n",
    "\n",
    "6. **Compilación**: el modelo se compila con un optimizador, una función de pérdida y la métrica deseada para su evaluación.\n",
    "\n",
    "7. **Resumen del modelo**: Esto imprime un resumen de la arquitectura del modelo.\n",
    "\n",
    "Recuerde ajustar los parámetros (por ejemplo, forma de entrada, número de clases, tamaños de filtro, etc.) de acuerdo con su conjunto de datos específico y su tarea de clasificación.\n",
    "\n",
    "Además, necesitará un conjunto de datos para entrenar y evaluar el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive',force_remount=True)\n",
    "#root_dir = '/content/gdrive/My Drive/Colab Notebooks/'\n",
    "#import sys\n",
    "#sys.append(root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d59787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalize data .... \n",
      "[INFO] datos cargados, normalizados y redimensionados a la forma: num imagenes x filas x columnas x canales\n",
      " \n",
      "Etiquetas o clases: \n",
      " [0 1 2 3 4 5 6 7 8 9] \n",
      "Datos por clase \n",
      " [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#load data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot       as plt\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from process import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "root_dir = ''\n",
    "X_train, y_train = loadlocal_mnist(\n",
    "        images_path=root_dir+'MNIST/train-images-idx3-ubyte', \n",
    "        labels_path=root_dir+'MNIST/train-labels-idx1-ubyte')\n",
    "\n",
    "X_test, y_test = loadlocal_mnist(\n",
    "        images_path=root_dir+'MNIST/t10k-images-idx3-ubyte', \n",
    "        labels_path=root_dir+'MNIST/t10k-labels-idx1-ubyte')\n",
    "\n",
    "\n",
    "print('[INFO] Normalize data .... ')\n",
    "#dividir entre 255\n",
    "X_train=X_train/255\n",
    "X_test=X_test/255        \n",
    "#definir el algoritmo para procesar imagenes\n",
    "#normalizer = whiten()\n",
    "normalizer = zca()\n",
    "\n",
    "#entrenar el algoritmo de normalizar\n",
    "normalizer.fit(X_train)\n",
    "\n",
    "#aplicar a los dos conjuntos\n",
    "X_train = normalizer.transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"[INFO] datos cargados, normalizados y redimensionados a la forma: num imagenes x filas x columnas x canales\")\n",
    "labels,count_class = np.unique(y_train,return_counts=True)\n",
    "print(' \\nEtiquetas o clases: \\n %s \\nDatos por clase \\n %s' % (labels,count_class))\n",
    "num_classes = labels.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0591c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(15, 5))\n",
    "index = 5000\n",
    "print(y_train[index])\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_train[index],cmap=\"gray\");\n",
    "plt.subplot(122)\n",
    "_,_,_=plt.hist(X_train[index].ravel(),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e169da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                102464    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121,930\n",
      "Trainable params: 121,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define el modelo CNN\n",
    "model = models.Sequential()\n",
    "\n",
    "# Primera capa convolucional\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Segunda capa convolucional\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten Layer : permite concatenar las caracteristicas en un vector unidimensional (aplanar)\n",
    "#sirve para preparar los datos de entrada a una red completamente conectada (FC - fully connected)\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully Connected Layer\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Capa de salida\n",
    "model.add(layers.Dense(10, activation='softmax'))  # En este caso se tienen 10 clases \n",
    "\n",
    "# Compile el modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# mostar el resumen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05640c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.1978 - accuracy: 0.9427\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.0511 - accuracy: 0.9842\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.0358 - accuracy: 0.9887\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.0265 - accuracy: 0.9914\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.0206 - accuracy: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c314a1fbe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding\n",
    "train_labels = to_categorical(y_train, num_classes=num_classes)\n",
    "test_labels = to_categorical(y_test, num_classes=num_classes)\n",
    "#entrenar el modelo\n",
    "model.fit(X_train, train_labels, epochs=5, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3120be45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0346 - accuracy: 0.9891\n",
      "Accuracy: 98.91\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, test_labels)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51028c5",
   "metadata": {},
   "source": [
    "### Caso 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb469c",
   "metadata": {},
   "source": [
    "Ejemplo de una red neuronal convolucional residual (ResNet) de tres capas que utiliza TensorFlow para clasificar imágenes. ResNet utiliza conexiones de salto (o atajos) para abordar el problema del gradiente que desaparece y mejorar el entrenamiento de redes profundas.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    # Atajo o Shortcut: es la entrada al bloque, se usa para sumarlo al final con\n",
    "    #el resultado de pasar esta misma entrada por las capas convolucionales\n",
    "    shortcut = x\n",
    "    \n",
    "    # Primer capa convolucional\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # Segunda capa convolucional\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Sumar el atajo con la salida de las capas convolucionales , generar la salida del bloque\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Definir el modelo ResNet\n",
    "def build_resnet():\n",
    "    input_layer = layers.Input(shape=(64, 64, 3))\n",
    "    \n",
    "    # Capa convolucional inicial\n",
    "    x = layers.Conv2D(32, 3, padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # Agregar los bloques residuales, en este caso se itera por 3 ciclos, para pasar x \n",
    "    # por 3 bloques identicos\n",
    "    for _ in range(3):  # Three residual blocks\n",
    "        x = residual_block(x, 32)\n",
    "    \n",
    "    # Pooling promedi\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Fully Connected \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    # Output Layer - capa de salida\n",
    "    output_layer = layers.Dense(10, activation='softmax')(x)  # Suponiendo que tenemos 10 clases\n",
    "    \n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = build_resnet()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# imprimir resumen\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "**Explicación**:\n",
    "\n",
    "1. **Función de bloque residual**: `residual_block` define un único bloque residual. Consta de dos capas convolucionales con normalización por lotes y activación ReLU, y una conexión directa. La salida del bloque es la suma de elementos del atajo y el resultado de las dos convoluciones.\n",
    "\n",
    "`BatchNormalization()` es una técnica utilizada en redes neuronales para normalizar las activaciones de una capa, lo que contribuye a una mayor estabilidad y velocidad de entrenamiento.\n",
    "\n",
    "La normalización por lotes (Batch Normalization) se aplica en una capa de una red neuronal después de la operación de convolución o la operación lineal. Consiste en dos pasos principales:\n",
    "\n",
    "    a. Normalización: Se calcula la media y la desviación estándar de las activaciones en el lote de entrenamiento actual.\n",
    "\n",
    "    b. Escala y Desplazamiento: Las activaciones se normalizan restando la media y dividiendo por la desviación estándar. Luego, se escalan y se desplazan utilizando dos parámetros aprendibles (gamma y beta). Esto permite que la red aprenda la mejor representación de los datos para el problema en cuestión.\n",
    "\n",
    "La normalización por lotes ayuda a combatir el problema de la desvanecimiento/explotación del gradiente en redes profundas.\n",
    "\n",
    "\n",
    "2. **Bloques residuales en el modelo**: En la función `build_resnet`, comenzamos con una capa convolucional inicial, seguida de tres bloques residuales.\n",
    "\n",
    "3. **Agrupación promedio global - GlobalAveragePooling**: después de los bloques residuales, utilizamos la agrupación promedio global para convertir las dimensiones espaciales en un vector unidimensional.\n",
    "\n",
    "4. **Capa completamente conectada - Fully connected**: los datos aplanados pasan a través de una capa completamente conectada.\n",
    "\n",
    "5. **Capa de salida - Output Layer**: la capa final utiliza la función de activación softmax para generar probabilidades para cada clase.\n",
    "\n",
    "6. **Compilación**: el modelo se compila con un optimizador, una función de pérdida y la métrica deseada para su evaluación.\n",
    "\n",
    "7. **Resumen del modelo**: Esto imprime un resumen de la arquitectura del modelo.\n",
    "\n",
    "Recuerde ajustar los parámetros (por ejemplo, forma de entrada, número de clases, tamaños de filtro, etc.) de acuerdo con su conjunto de datos específico y su tarea de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbca9649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 28, 28, 32)   320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 28, 28, 32)  128         ['conv2d_2[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 28, 28, 32)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 28, 28, 32)   9248        ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 28, 28, 32)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 28, 28, 32)   9248        ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 28, 28, 32)   0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  're_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 28, 28, 32)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 28, 28, 32)   9248        ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 28, 28, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 28, 28, 32)   9248        ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 28, 28, 32)   0           ['batch_normalization_4[0][0]',  \n",
      "                                                                  're_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 28, 28, 32)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 28, 28, 32)   9248        ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 28, 28, 32)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 28, 28, 32)   9248        ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 28, 28, 32)   0           ['batch_normalization_6[0][0]',  \n",
      "                                                                  're_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 28, 28, 32)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 32)          0           ['re_lu_6[0][0]']                \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           2112        ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           650         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 59,466\n",
      "Trainable params: 59,018\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    # Atajo o Shortcut: es la entrada al bloque, se usa para sumarlo al final con\n",
    "    #el resultado de pasar esta misma entrada por las capas convolucionales\n",
    "    shortcut = x\n",
    "    \n",
    "    # Primer capa convolucional\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # Segunda capa convolucional\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Sumar el atajo con la salida de las capas convolucionales , generar la salida del bloque\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Definir el modelo ResNet\n",
    "def build_resnet():\n",
    "    input_layer = layers.Input(shape=(28, 28, 1))\n",
    "    \n",
    "    # Capa convolucional inicial\n",
    "    x = layers.Conv2D(32, 3, padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # Agregar los bloques residuales, en este caso se itera por 3 ciclos, para pasar x \n",
    "    # por 3 bloques identicos\n",
    "    for _ in range(3):  # Three residual blocks\n",
    "        x = residual_block(x, 32)\n",
    "    \n",
    "    # Pooling promedi\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Fully Connected \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    # Output Layer - capa de salida\n",
    "    output_layer = layers.Dense(10, activation='softmax')(x)  # Suponiendo que tenemos 10 clases\n",
    "    \n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = build_resnet()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# imprimir resumen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5ab35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "600/600 [==============================] - 92s 151ms/step - loss: 0.4505 - accuracy: 0.8789\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.0808 - accuracy: 0.9778\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 93s 155ms/step - loss: 0.0593 - accuracy: 0.9821\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 88s 147ms/step - loss: 0.0493 - accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.0421 - accuracy: 0.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c316c5f520>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding\n",
    "train_labels = to_categorical(y_train, num_classes=num_classes)\n",
    "test_labels = to_categorical(y_test, num_classes=num_classes)\n",
    "#entrenar el modelo\n",
    "model.fit(X_train, train_labels, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cb4686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 14ms/step - loss: 0.1317 - accuracy: 0.9607\n",
      "Accuracy: 96.07\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, test_labels)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc68a175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06e4d7e6",
   "metadata": {},
   "source": [
    "### Caso 3:\n",
    "\n",
    "La arquitectura VGG es conocida por sus profundas capas convolucionales. A continuación se muestra un ejemplo de una red neuronal convolucional (CNN) de tres capas que utiliza la arquitectura VGG con TensorFlow:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_vgg(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Boque 1\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Boque 2\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Boque 3\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Flatten and Fully Connected Layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Crear el modelo VGG\n",
    "input_shape = (224, 224, 3)  # Ajustar segun necesidades\n",
    "num_classes = 10  # Asumir 10 clases\n",
    "\n",
    "model = build_vgg(input_shape, num_classes)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Mostrar el resumen\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "**Explicación**\n",
    "1. **Bloque 1**: Contiene dos capas convolucionales con 64 filtros cada una, seguidas de una agrupación máxima.\n",
    "\n",
    "2. **Bloque 2**: Contiene dos capas convolucionales con 128 filtros cada una, seguidas de una agrupación máxima.\n",
    "\n",
    "3. **Bloque 3**: Contiene tres capas convolucionales con 256 filtros cada una, seguidas de una agrupación máxima.\n",
    "\n",
    "4. **Capas aplanadas y completamente conectadas - Flatten and Fully Connected Layers**: después de los bloques convolucionales, los datos se aplanan y pasan a través de dos capas completamente conectadas con 4096 unidades cada una.\n",
    "\n",
    "5. **Capa de salida - Output Layer**: la capa final utiliza la función de activación softmax para generar probabilidades para cada clase.\n",
    "\n",
    "Recuerde ajustar `input_shape` de acuerdo con las dimensiones de sus imágenes y configurar `num_classes` para que coincida con el número de clases en su tarea de clasificación. Además, prepare su conjunto de datos para entrenamiento y evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be46d3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 64)        640       \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 7, 7, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 7, 7, 256)         590080    \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 7, 7, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 3, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4096)              9441280   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,997,898\n",
      "Trainable params: 27,997,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_vgg(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Boque 1\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Boque 2\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Boque 3\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Flatten and Fully Connected Layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Crear el modelo VGG\n",
    "input_shape = (28, 28, 1)  # Ajustar segun necesidades\n",
    "num_classes = 10  # Asumir 10 clases\n",
    "\n",
    "model = build_vgg(input_shape, num_classes)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Mostrar el resumen\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05290257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "600/600 [==============================] - 252s 420ms/step - loss: 0.2149 - accuracy: 0.9297\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 245s 408ms/step - loss: 0.0533 - accuracy: 0.9844\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 245s 408ms/step - loss: 0.0401 - accuracy: 0.9879\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 249s 414ms/step - loss: 0.0328 - accuracy: 0.9908\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 236s 394ms/step - loss: 0.0327 - accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c31b6fe340>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding\n",
    "train_labels = to_categorical(y_train, num_classes=num_classes)\n",
    "test_labels = to_categorical(y_test, num_classes=num_classes)\n",
    "#entrenar el modelo\n",
    "model.fit(X_train, train_labels, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a5fcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 14s 43ms/step - loss: 0.0327 - accuracy: 0.9917\n",
      "Accuracy: 99.17\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, test_labels)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8ef8c",
   "metadata": {},
   "source": [
    "### Caso 4:\n",
    "Un ejemplo de una red neuronal convolucional (CNN) de tres capas que utiliza la arquitectura VGG con convoluciones separables en TensorFlow:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_vgg_separable(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Bloque 1 con Separable Convolutions\n",
    "    model.add(layers.SeparableConv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(layers.SeparableConv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Bloque 2 con Separable Convolutions\n",
    "    model.add(layers.SeparableConv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.SeparableConv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Bloque 3 con Separable Convolutions\n",
    "    model.add(layers.SeparableConv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.SeparableConv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.SeparableConv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Flatten and Fully Connected Layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Crear el modelo VGG\n",
    "input_shape = (224, 224, 3)  # ajustar de acuerdo a necesidades\n",
    "num_classes = 10  # Assumir 10 clases\n",
    "\n",
    "model = build_vgg_separable(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Imprimir el resumen\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "En este ejemplo, se utilizan las capas `SeparableConv2D` en lugar de las capas convolucionales normales. Las convoluciones separables son computacionalmente más eficientes que las convoluciones tradicionales, ya que factorizan la operación en una convolución profunda y una convolución puntual. Esto puede conducir a una reducción en la cantidad de parámetros y el costo computacional.\n",
    "\n",
    "Recuerde ajustar `input_shape` de acuerdo con las dimensiones de sus imágenes y configurar `num_classes` para que coincida con el número de clases en su tarea de clasificación. Además, prepare su conjunto de datos para entrenamiento y evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eb054a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " separable_conv2d (Separable  (None, 28, 28, 64)       137       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " separable_conv2d_1 (Separab  (None, 28, 28, 64)       4736      \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " separable_conv2d_2 (Separab  (None, 14, 14, 128)      8896      \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " separable_conv2d_3 (Separab  (None, 14, 14, 128)      17664     \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " separable_conv2d_4 (Separab  (None, 7, 7, 256)        34176     \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " separable_conv2d_5 (Separab  (None, 7, 7, 256)        68096     \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " separable_conv2d_6 (Separab  (None, 7, 7, 256)        68096     \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 3, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4096)              9441280   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,465,363\n",
      "Trainable params: 26,465,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_vgg_separable(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Bloque 1 con Separable Convolutions\n",
    "    model.add(layers.SeparableConv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(layers.SeparableConv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Bloque 2 con Separable Convolutions\n",
    "    model.add(layers.SeparableConv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.SeparableConv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Bloque 3 con Separable Convolutions\n",
    "    model.add(layers.SeparableConv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.SeparableConv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.SeparableConv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Flatten and Fully Connected Layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Crear el modelo VGG\n",
    "input_shape = (28, 28, 1)  # ajustar de acuerdo a necesidades\n",
    "num_classes = 10  # Assumir 10 clases\n",
    "\n",
    "model = build_vgg_separable(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Imprimir el resumen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ca7f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "600/600 [==============================] - 229s 381ms/step - loss: 2.3017 - accuracy: 0.1113\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 224s 374ms/step - loss: 2.3015 - accuracy: 0.1124\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 225s 374ms/step - loss: 2.3014 - accuracy: 0.1124\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 235s 391ms/step - loss: 2.3013 - accuracy: 0.1124\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 237s 394ms/step - loss: 2.3013 - accuracy: 0.1124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c31b3ce250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding\n",
    "train_labels = to_categorical(y_train, num_classes=num_classes)\n",
    "test_labels = to_categorical(y_test, num_classes=num_classes)\n",
    "#entrenar el modelo\n",
    "model.fit(X_train, train_labels, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b1613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 25ms/step - loss: 2.3011 - accuracy: 0.1135\n",
      "Accuracy: 11.35\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, test_labels)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b67f3",
   "metadata": {},
   "source": [
    "## Resumen de resultados:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
