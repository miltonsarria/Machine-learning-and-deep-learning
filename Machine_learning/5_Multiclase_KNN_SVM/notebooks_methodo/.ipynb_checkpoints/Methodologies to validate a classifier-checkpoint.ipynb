{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodologies to validate a classifier \n",
    "\n",
    "Adjusting the parameters of a model to make some kind of prediction and evaluate the system on the same data that have been used for training is a methodological error.\n",
    "\n",
    "A model that repeats the labels of the data you just saw during training might do a perfect job, but what about data you haven't seen? will the output correspond to something useful? This situation is known as overtraining.\n",
    "\n",
    "To avoid this type of problem and have a better idea of the real behavior of the system (classification or regression) processing data that is not known. We partition the data into two subsets: X_train and X_test. This allows the automated system to be trained using the X_train data and evaluated using the X_test data. \n",
    "\n",
    "\n",
    "There is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different strategies, we will use  the tools available in sk-learn for this purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the IRIS database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load iris\n",
    "################################################################################################\n",
    "def loadIris():\n",
    "    #\n",
    "    file_name='datosML/iris.data'\n",
    "    labels={'Iris-setosa':0,'Iris-versicolor':1,'Iris-virginica':2}\n",
    "\n",
    "    hf = open(file_name,'r')\n",
    "    lines=hf.readlines()\n",
    "    hf.close()\n",
    "    ##############################\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for line in lines:\n",
    "       #\n",
    "        data = line[:-1].split(',')\n",
    "        y.append(labels[data[-1]])\n",
    "        x.append([float(i) for i in data[:-1]])\n",
    "\n",
    "    #\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "#Verify data\n",
    "X,Y=loadIris()\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## K-fold\n",
    "KFold divides all the samples in $k$ groups of samples, called folds (if $k=n$ , this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using $k-1$ folds, and the fold left out is used for test.\n",
    "<img src=\"img_ex/kfolds.png\" width=\"450\">\n",
    "\n",
    "There is a problem....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "test labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "test labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "test labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "test labels:  [0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "test labels:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "test labels:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "test labels:  [1 1 1 1 1 1 1 1 1 1 2 2 2 2 2]\n",
      "test labels:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "test labels:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "test labels:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "X,Y=loadIris()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "#10 fold\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        print(\"test labels: \", y_test)\n",
    "        #print(\"train labels: \", y_train)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To solve the problem, we can shufle the dataset before applying the algorithm above. We use a random permutation of the indices as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4be04b3e53fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#values from 10 to 20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"valores de x:              \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#shufle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#values from 10 to 20\n",
    "x=np.arange(10,20)\n",
    "print(\"valores de x:              \", x)\n",
    "#shufle\n",
    "y=np.random.permutation(10)\n",
    "print(\"nuevos indices para x:     \", y)\n",
    "\n",
    "#use the new indexes\n",
    "x=x[y]\n",
    "print(\"valores de x, desordenados:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same for the IRIS dataset\n",
    "ind=np.random.permutation(Y.size)\n",
    "\n",
    "\n",
    "X=X[ind,:]\n",
    "Y=Y[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test labels:  [0 2 2 2 1 2 2 0 0 0 1 1 2 2 0]\n",
      "test labels:  [0 1 0 2 0 0 2 2 0 0 2 1 1 0 1]\n",
      "test labels:  [2 0 1 0 2 0 0 1 1 0 1 1 0 1 0]\n",
      "test labels:  [1 2 0 0 1 0 1 0 2 0 2 1 1 0 0]\n",
      "test labels:  [2 1 0 2 1 0 2 1 1 2 0 1 2 0 2]\n",
      "test labels:  [1 1 2 0 0 2 2 2 2 2 2 1 2 2 0]\n",
      "test labels:  [0 0 0 1 0 1 1 0 0 2 1 2 1 1 1]\n",
      "test labels:  [0 1 1 2 0 1 2 1 1 0 1 2 2 1 2]\n",
      "test labels:  [2 2 1 1 2 2 2 2 0 1 2 1 1 1 2]\n",
      "test labels:  [0 0 1 0 0 1 2 0 0 1 2 2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        print(\"test labels: \", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a classifier: \n",
    "\n",
    "1. Logistic regression\n",
    "2. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación de rendimiento de los dos clasificadores:\n",
      "\n",
      "Logistic regression: promedio = 90.000000, std = 11.642833\n",
      "KNN                : promedio = 94.666667, std = 6.531973\n"
     ]
    }
   ],
   "source": [
    "#generamos dos vectores de ceros para guardar la tasa de acierto (% de muestras clasificadas correctamente) de\n",
    "#los dos clasificadors, uno pada caso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#numero de folds\n",
    "k=10 \n",
    "\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        #CLF 1\n",
    "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='ovr')\n",
    "        clf1.fit(X_train, y_train)\n",
    "        #evaluate\n",
    "        yp1 = clf1.predict(X_test)\n",
    "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
    "        \n",
    "        #Clf2\n",
    "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "        clf2.fit(X_train, y_train)\n",
    "        \n",
    "        #Evaluate\n",
    "        yp2 = clf2.predict(X_test)\n",
    "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
    "                \n",
    "acc1=np.array(acc1)\n",
    "acc2=np.array(acc2)\n",
    "\n",
    "\n",
    "print(\"Logistic regression: average = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
    "print(\"KNN                : average = %f, std = %f\"% (acc2.mean(), acc2.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 93.33333333,  86.66666667,  93.33333333, 100.        ,\n",
       "        93.33333333,  93.33333333, 100.        ,  93.33333333,\n",
       "       100.        ,  86.66666667])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out (LOO)\n",
    "\n",
    "LeaveOneOut (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for  $n$ samples, we have different training sets and $n$ different tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "Comparación de rendimiento de los dos clasificadores:\n",
      "\n",
      "Logistic regression: tasa acierto =  94.0\n",
      "KNN                : tasa acierto =  96.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "\n",
    "X,Y=loadIris()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "#index generator\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for train, test in loo.split(X):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = Y[train], Y[test]\n",
    "        #Clf 1\n",
    "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='ovr')\n",
    "        clf1.fit(X_train, y_train)\n",
    "        #eval\n",
    "        yp1 = clf1.predict(X_test)\n",
    "        acc1.append(yp1==y_test)\n",
    "        \n",
    "        #clf2\n",
    "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "        clf2.fit(X_train, y_train)\n",
    "        \n",
    "        #eval\n",
    "        yp2 = clf2.predict(X_test)\n",
    "        acc2.append(yp2==y_test)\n",
    "                \n",
    "acc1=np.array(acc1).sum()/len(acc1)*100\n",
    "acc2=np.array(acc2).sum()/len(acc2)*100\n",
    "\n",
    "\n",
    "\n",
    "print(\"Logistic regression: accuracy = \", acc1)\n",
    "print(\"KNN                : accuracy = \", acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede notar que la tasa de acierto es igual a la que se obtuvo con KFOLDS, sin embargo en este caso no es posible calcular un promedio o una desviación estandard, pues en cada iteración solo habia una muestra, por lo que el acierto es 100% si esa muestra se clasifica bien, o 0% si se clasifica mal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random cross-validation = Shuffle & Split\n",
    "\n",
    "The ShuffleSplit iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.\n",
    "\n",
    "It is possible to control the randomness for reproducibility of the results by explicitly seeding the random_state pseudo random number generator.\n",
    "\n",
    "\n",
    "ShuffleSplit(n_splits=20, test_size=0.3, random_state=0)\n",
    "\n",
    "n_splits number of splits\n",
    "\n",
    "test_size portion of data to use in testing\n",
    "\n",
    "70% - 30%\n",
    "\n",
    "Here is a visualization of the cross-validation behavior. Note that ShuffleSplit is not affected by classes or groups.\n",
    "\n",
    "<img src=\"img_ex/random.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "Comparación de rendimiento de los dos clasificadores:\n",
      "\n",
      "Logistic regression: promedio = 93.333333, std = 2.981424\n",
      "KNN                : promedio = 96.222222, std = 2.989694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "\n",
    "X,Y=loadIris()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "#\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
    "for train_index, test_index in ss.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        #clf1\n",
    "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='ovr')\n",
    "        clf1.fit(X_train, y_train)\n",
    "        #evaluate\n",
    "        yp1 = clf1.predict(X_test)\n",
    "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
    "        \n",
    "        #clf2\n",
    "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "        clf2.fit(X_train, y_train)\n",
    "        \n",
    "        #Evaluate\n",
    "        yp2 = clf2.predict(X_test)\n",
    "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
    "\n",
    "               \n",
    "acc1=np.array(acc1)\n",
    "acc2=np.array(acc2)\n",
    "\n",
    "print(\"Logistic regression: Average = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
    "print(\"KNN                : Average = %f, std = %f\"% (acc2.mean(), acc2.std()))    \n",
    "del X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eexample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "## Funcion para cargar los datos\n",
    "def loadDiab():\n",
    "    #define el nombre del archivo y las etiquetas para cada tipo de flor\n",
    "    file_name='datosML/diabetes.data'\n",
    "    \n",
    "\n",
    "    hf = open(file_name,'r')\n",
    "    lines=hf.readlines()\n",
    "    hf.close()\n",
    "    ##############################\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for line in lines:\n",
    "       #de cada linea tomar por separado cada numero y la etiqueta\n",
    "        data = line[:-1].split(',')\n",
    "        y.append(int(data[-1]))\n",
    "        x.append([float(i) for i in data[:-1]])\n",
    "\n",
    "    #convertir la lista a un arreglo numpy\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    return x,y\n",
    "\n",
    "X,Y=loadDiab()\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación de rendimiento de los dos clasificadores:\n",
      "\n",
      "Logistic regression: promedio = 77.611073, std = 6.362026\n",
      "KNN                : promedio = 73.564593, std = 5.020999\n"
     ]
    }
   ],
   "source": [
    "#se realiza primero una permutación aleatoria de los datos llamaremos a los nuevos indices ind\n",
    "ind=np.random.permutation(Y.size)\n",
    "\n",
    "#tomamos el nuevo orden indicado por la permutación aleatoria\n",
    "X=X[ind,:]\n",
    "Y=Y[ind]\n",
    "\n",
    "#numero de folds\n",
    "k=10 \n",
    "\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        #clasficador 1\n",
    "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='ovr')\n",
    "        clf1.fit(X_train, y_train)\n",
    "        #evaluar clf1 y guardar el resultado en acc1\n",
    "        yp1 = clf1.predict(X_test)\n",
    "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
    "        \n",
    "        #clasificador 2\n",
    "        clf2 = KNeighborsClassifier(n_neighbors=9)\n",
    "        clf2.fit(X_train, y_train)\n",
    "        \n",
    "        #evaluar clf2 y guardar el resultado en acc2\n",
    "        yp2 = clf2.predict(X_test)\n",
    "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
    "                \n",
    "acc1=np.array(acc1)\n",
    "acc2=np.array(acc2)\n",
    "\n",
    "print(\"Comparación de rendimiento de los dos clasificadores:\\n\")        \n",
    "print(\"Logistic regression: promedio = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
    "print(\"KNN                : promedio = %f, std = %f\"% (acc2.mean(), acc2.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out (LOO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n",
      "Comparación de rendimiento de los dos clasificadores:\n",
      "\n",
      "Logistic regression: Tasa acierto =  77.60416666666666\n",
      "KNN                : Tasa acierto =  69.40104166666666\n"
     ]
    }
   ],
   "source": [
    "X,Y=loadDiab()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "\n",
    "\n",
    "#crear el generador de indices\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for train, test in loo.split(X):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = Y[train], Y[test]\n",
    "        #clasficador 1\n",
    "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='ovr')\n",
    "        clf1.fit(X_train, y_train)\n",
    "        #evaluar clf1 y guardar el resultado en acc1, en este caso como es una sola muestra no es necesario\n",
    "        #sumar ni dividir entre 100\n",
    "        yp1 = clf1.predict(X_test)\n",
    "        acc1.append(yp1==y_test)\n",
    "        \n",
    "        #clasificador 2\n",
    "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "        clf2.fit(X_train, y_train)\n",
    "        \n",
    "        #evaluar clf2 y guardar el resultado en acc2\n",
    "        yp2 = clf2.predict(X_test)\n",
    "        acc2.append(yp2==y_test)\n",
    "                \n",
    "acc1=np.array(acc1).sum()/len(acc1)*100\n",
    "acc2=np.array(acc2).sum()/len(acc2)*100\n",
    "\n",
    "\n",
    "print(\"Comparación de rendimiento de los dos clasificadores:\\n\")        \n",
    "print(\"Logistic regression: Tasa acierto = \", acc1)\n",
    "print(\"KNN                : Tasa acierto = \", acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random cross-validation = Shuffle & Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n",
      "Comparación de rendimiento de los dos clasificadores:\n",
      "\n",
      "Logistic regression: promedio = 77.965368, std = 2.286190\n",
      "KNN                : promedio = 69.826840, std = 1.764302\n"
     ]
    }
   ],
   "source": [
    "X,Y=loadDiab()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "\n",
    "#generador de indices\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
    "for train_index, test_index in ss.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        #clasficador 1\n",
    "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='ovr')\n",
    "        clf1.fit(X_train, y_train)\n",
    "        #evaluar clf1 y guardar el resultado en acc1\n",
    "        yp1 = clf1.predict(X_test)\n",
    "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
    "        \n",
    "        #clasificador 2\n",
    "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "        clf2.fit(X_train, y_train)\n",
    "        \n",
    "        #evaluar clf2 y guardar el resultado en acc2\n",
    "        yp2 = clf2.predict(X_test)\n",
    "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
    "\n",
    "               \n",
    "acc1=np.array(acc1)\n",
    "acc2=np.array(acc2)\n",
    "\n",
    "print(\"Comparación de rendimiento de los dos clasificadores:\\n\")        \n",
    "print(\"Logistic regression: promedio = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
    "print(\"KNN                : promedio = %f, std = %f\"% (acc2.mean(), acc2.std()))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation iterators with stratification based on class labels.\n",
    "Some classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use stratified sampling as implemented in StratifiedKFold and StratifiedShuffleSplit to ensure that relative class frequencies is approximately preserved in each train and validation fold.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified k-fold\n",
    "\n",
    "StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n",
      "Comparación de rendimiento de los dos clasificadores:\n",
      "\n",
      "Logistic regression: promedio = 77.347915, std = 3.574822\n",
      "KNN                : promedio = 70.305878, std = 3.763358\n"
     ]
    }
   ],
   "source": [
    "#Stratified k-fold\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "X,Y=loadDiab()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "\n",
    "#se genera el generador de indices de forma estratificada\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "   \n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        #clasficador 1\n",
    "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='ovr')\n",
    "        clf1.fit(X_train, y_train)\n",
    "        #evaluar clf1 y guardar el resultado en acc1\n",
    "        yp1 = clf1.predict(X_test)\n",
    "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
    "        \n",
    "        #clasificador 2\n",
    "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "        clf2.fit(X_train, y_train)\n",
    "        \n",
    "        #evaluar clf2 y guardar el resultado en acc2\n",
    "        yp2 = clf2.predict(X_test)\n",
    "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
    "                \n",
    "acc1=np.array(acc1)\n",
    "acc2=np.array(acc2)\n",
    "\n",
    "print(\"Comparación de rendimiento de los dos clasificadores:\\n\")        \n",
    "print(\"Logistic regression: promedio = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
    "print(\"KNN                : promedio = %f, std = %f\"% (acc2.mean(), acc2.std()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Shuffle Split\n",
    "\n",
    "StratifiedShuffleSplit is a variation of ShuffleSplit, which returns stratified splits, i.e which creates splits by preserving the same percentage for each target class as in the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n",
      "Comparación de rendimiento de los dos clasificadores:\n",
      "\n",
      "Logistic regression: promedio = 76.753247, std = 5.241672\n",
      "KNN                : promedio = 65.844156, std = 4.269943\n"
     ]
    }
   ],
   "source": [
    "#Stratified ShuffleSplit \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X,Y=loadDiab()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "\n",
    "#se genera el generador de indices de forma estratificada\n",
    "sss = StratifiedShuffleSplit(n_splits=10)\n",
    "   \n",
    "for train_index, test_index in sss.split(X,Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        #clasficador 1\n",
    "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='ovr')\n",
    "        clf1.fit(X_train, y_train)\n",
    "        #evaluar clf1 y guardar el resultado en acc1\n",
    "        yp1 = clf1.predict(X_test)\n",
    "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
    "        \n",
    "        #clasificador 2\n",
    "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
    "        clf2.fit(X_train, y_train)\n",
    "        \n",
    "        #evaluar clf2 y guardar el resultado en acc2\n",
    "        yp2 = clf2.predict(X_test)\n",
    "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
    "                \n",
    "acc1=np.array(acc1)\n",
    "acc2=np.array(acc2)\n",
    "\n",
    "print(\"Comparación de rendimiento de los dos clasificadores:\\n\")        \n",
    "print(\"Logistic regression: promedio = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
    "print(\"KNN                : promedio = %f, std = %f\"% (acc2.mean(), acc2.std()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
