{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, you will discover why machine learning practitioners should study probability to improve their skills and capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability is a field of mathematics that quantifies\n",
    "uncertainty. Machine learning is about developing predictive modeling from uncertain data. \n",
    "\n",
    "Uncertainty means working with imperfect or incomplete information. Uncertainty is fundamental to the field of machine learning, yet it is one of the aspects that causes the most difficulty for beginners\n",
    "\n",
    "\n",
    "There are three main sources of uncertainty in machine learning; they are:\n",
    "\n",
    "1. Noise in observations, e.g. measurement errors and random noise.\n",
    "2. Incomplete coverage of the domain, e.g. you can never observe all data.\n",
    "3. Imperfect model of the problem, e.g. all models have errors, some are useful.\n",
    "\n",
    "Uncertainty in applied machine learning is managed using probability.\n",
    "\n",
    "1. Probability and statistics help us to understand and quantify the expected value and variability of variables in our observations from the domain.\n",
    "2. Probability helps to understand and quantify the expected distribution and density of observations in the domain.\n",
    "3. Probability helps to understand and quantify the expected capability and variance in performance of our predictive models when applied to new data.\n",
    "\n",
    "\n",
    "This is the bedrock of machine learning. On top of that, we may need models to predict a probability, we may use probability to develop predictive models, and we may\n",
    "use probabilistic frameworks to train predictive models.\n",
    "\n",
    "\n",
    "\n",
    "## You must list three reasons why you want to learn probability in the context of machine learning. These may be related to some of the reasons above, or they may be your own personal motivations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1-\n",
    " \n",
    " 2-\n",
    " \n",
    " 3-\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Types of Probability #\n",
    "\n",
    "Probability quantifies the likelihood of an event. Specifically, it quantifies how likely a specific outcome is for a random variable, such as the flip of a coin,\n",
    "the roll of a die, or drawing a playing card from a deck.\n",
    "\n",
    "We can discuss the probability of just two events: the probability of event $A$ for variable $X$ and event $B$ for variable $Y$ , which in shorthand is $X = A$ and $Y = B$, and that the two variables are related or dependent in some way. As such, there are three main types of probability we might want to consider.\n",
    "\n",
    "## Joint Probability ##\n",
    "\n",
    "We may be interested in the probability of two simultaneous events, like the outcomes of two\n",
    "different random variables. For example, the joint probability of event A and event B is written\n",
    "formally as: \n",
    "\n",
    "$P(A \\; \\cap\\; B)$\n",
    "\n",
    "The joint probability for events A and B is calculated as the probability of event A given event B multiplied by the probability of event B. This can be stated formally as follows:\n",
    "\n",
    " $P(A \\; \\cap\\; B) = P(A \\; \\mid \\; B) * P(B)$\n",
    "\n",
    "## Marginal Probability ##\n",
    "We may be interested in the probability of an event for one random variable, irrespective of the outcome of another random variable. \n",
    "There is no special notation for marginal probability; it is just the sum or union over all the probabilities of all events for the second variable for a given\n",
    "fixed event for the first variable.\n",
    "\n",
    "$P(X=A) = \\sum_{i=1}^n P(X=A, Y=y_i)$\n",
    "\n",
    "## Conditional Probability## \n",
    "\n",
    "We may be interested in the probability of an event given the occurrence of another event.\n",
    "\n",
    "For example, the conditional probability of event A given event B is written formally as:\n",
    "\n",
    "$P(A \\; \\mid \\; B)$\n",
    "\n",
    "The conditional probability for events A given event B can be calculated using the joint probability of the events as follows:\n",
    "\n",
    "$P(A \\; \\mid \\; B) = \\frac{P(A \\; \\cap \\; B)}{  P(B)}$\n",
    "\n",
    "\n",
    "### Your Task### \n",
    "if a family has two children and the oldest is a boy, what is the probability of this\n",
    "family having two sons? This is called the Boy or Girl Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distributions#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In probability, a random variable can take on one of many possible values, e.g. events from the state space. A\n",
    "specific value or set of values for a random variable can be assigned a probability. There are\n",
    "two main classes of random variables.\n",
    "\n",
    "1. Discrete Random Variable. Values are drawn from a finite set of states.\n",
    "\n",
    "2. Continuous Random Variable. Values are drawn from a range of real-valued numerical values.\n",
    "\n",
    "A discrete random variable has a finite set of states; for example, the colors of a car. A continuous random variable has a range of numerical values; for example, the height of humans. A probability distribution is a summary of probabilities for the values of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Probability Distributions### \n",
    "A discrete probability distribution summarizes the  probabilities for a discrete random variable. Some examples of well-known discrete probability distributions include:\n",
    "\n",
    "- Poisson distribution.\n",
    "- Bernoulli and binomial distributions.\n",
    "- Multinoulli and multinomial distributions.\n",
    "\n",
    "### Continuous Probability Distributions### \n",
    "A continuous probability distribution summarizes the probability for a continuous random variable. Some examples of well-known continuous probability distributions include:\n",
    "- Normal or Gaussian distribution.\n",
    "- Exponential distribution.\n",
    "- Pareto distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Sample Gaussian Distribution### \n",
    "We can define a distribution with a mean of 50 and a standard deviation of 5 and sample random\n",
    "numbers from this distribution. We can achieve this using the normal() NumPy function. The\n",
    "example below samples and prints 10 numbers from this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,  25.,  63., 201., 285., 231., 141.,  43.,   6.,   2.]),\n",
       " array([33.73924652, 37.18755074, 40.63585495, 44.08415917, 47.53246339,\n",
       "        50.98076761, 54.42907182, 57.87737604, 61.32568026, 64.77398448,\n",
       "        68.22228869]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample a normal distribution\n",
    "from numpy.random import normal\n",
    "# define the distribution\n",
    "mu = 50\n",
    "sigma = 5\n",
    "n = 1000\n",
    "# generate the sample\n",
    "sample = normal(mu, sigma, n)\n",
    "#print(sample)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task\n",
    "you must develop an example to sample from a different continuous or discrete probability distribution function. For a bonus, you can plot the values on the x-axis and the\n",
    "probability on the y-axis for a given distribution to show the density of your chosen probability distribution function.\n",
    "\n",
    "https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier#\n",
    "\n",
    "In machine learning, we are often interested in a predictive modeling problem where we want to predict a class label for a given observation. One approach to solving this problem is to develop a probabilistic model. From a probabilistic perspective, we are interested in estimating the conditional probability of the class label given the observation, or the probability of class $y$\n",
    "given input data $X$.\n",
    "\n",
    "$P(y\\;\\mid\\;X)$\n",
    "\n",
    "\n",
    "Bayes Theorem provides an alternate and principled way for calculating the conditional probability using the reverse of the desired conditional probability, which is often simpler to calculate. The simple form of the calculation for Bayes Theorem is as follows:\n",
    "\n",
    "$P(A\\;\\mid\\;B) = \\frac{ P (B\\;\\mid\\;A) \\times P (A)}{P (B)}$\n",
    "\n",
    "Where the probability that we are interested in calculating $P(A\\;\\mid\\;B)$ is called the posterior\n",
    "probability and the marginal probability of the event $P (A)$ is called the prior. The direct application of Bayes Theorem for classification becomes intractable, especially as the number of variables or features $(n) $ increases. Instead, we can simplify the calculation and assume that each input variable is independent. Although dramatic, this simpler calculation often gives very good performance, even when the input variables are highly dependent. We can implement this from scratch by assuming a probability distribution for each separate input variable and calculating the probability of each specific input value belonging to each class and multiply the\n",
    "results together to give a score used to select the most likely class.\n",
    "\n",
    "\n",
    "$P(y_i\\;\\mid\\;x_1,x_2,x_3,\\cdots,x_n)=P(x_1\\;\\mid\\;y_i) \\times P(x_2\\;\\mid\\;y_i) \\times \\cdots \\times P(x_n \\;\\mid\\;y_i) \\times P(y_i)$\n",
    "\n",
    "\n",
    "\n",
    "The scikit-learn library provides an efficient implementation of the algorithm if we assume a\n",
    "Gaussian distribution for each input variable.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicted Probabilities:  [[1.00000000e+00 5.52387327e-30]]\n",
      " Predicted Class:  [0]\n",
      " Truth: y=0 \n"
     ]
    }
   ],
   "source": [
    "# example of gaussian naive bayes\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# define the model\n",
    "\n",
    "model = GaussianNB()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# select a single sample\n",
    "Xsample, ysample = [X[0]], y[0]\n",
    "# make a probabilistic prediction\n",
    "yhat_prob = model.predict_proba(Xsample)\n",
    "print( ' Predicted Probabilities: ' , yhat_prob)\n",
    "# make a classification prediction\n",
    "yhat_class = model.predict(Xsample)\n",
    "print( ' Predicted Class: ' , yhat_class)\n",
    "print( ' Truth: y=%d ' % ysample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must run the example and report the result, explaining each output. Try the algorithm on a real classification dataset, such as the popular toy classification problem of classifying iris flower species based on flower measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Cross-Entropy #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information theory is a field of study concerned with quantifying information for communication. The intuition behind quantifying information is the idea of measuring how much surprise there is in an event. Those events that are rare (low probability) are more surprising and therefore have more information than those events that are common (high probability).\n",
    "\n",
    "- Low Probability Event: High Information (surprising).\n",
    "- High Probability Event: Low Information (unsurprising).\n",
    "\n",
    "We can calculate the amount of information there is in an event using the probability of the even\n",
    "\n",
    "$I(x)=-\\log(P(x))$\n",
    "\n",
    "We can also quantify how much information there is in a random variable. This is called entropy and summarizes the amount of information required on average to represent events. Entropy can be calculated for a random variable $X$ with $K$ discrete states as follows:\n",
    "\n",
    "\n",
    "$H(X) = - \\sum_{i=1} ^K P(k_i) \\times \\log(P(k_i))$\n",
    "\n",
    "\n",
    "Cross-entropy is a measure of the difference between two probability distributions for a  given random variable or set of events. It is widely used as a loss function when optimizing classification models. It builds upon the idea of entropy and calculates the average number of bits required to represent or transmit an event from one distribution compared to the other distribution.\n",
    "\n",
    "\n",
    "$CH (P,Q) = -  \\sum ^{x \\in X} P(x) \\times \\log(Q(x))$\n",
    "\n",
    "\n",
    "Consider a random variable with three events as different colors. We may have two different probability distributions for this variable. We can calculate the cross-entropy between these two distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " H(P, Q): 3.288 bits \n",
      " H(Q, P): 2.906 bits \n"
     ]
    }
   ],
   "source": [
    "# example of calculating cross-entropy\n",
    "from math import log2\n",
    "# calculate cross-entropy\n",
    "def cross_entropy(p, q):\n",
    "    CE = 0\n",
    "    for i in range(len(p)):\n",
    "        CE = CE + p[i]*log2(q[i])\n",
    "    return -CE\n",
    "# define data\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "# calculate cross-entropy H(P, Q)\n",
    "ce_pq = cross_entropy(p, q)\n",
    "print( ' H(P, Q): %.3f bits ' % ce_pq)\n",
    "# calculate cross-entropy H(Q, P)\n",
    "ce_qp = cross_entropy(q, p)\n",
    "print( ' H(Q, P): %.3f bits ' % ce_qp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must run the example and describe the results and what they mean. For example, is the calculation of cross-entropy symmetrical?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Classifiers #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification predictive modeling problems involve predicting a class label given an input to the model. Given a classification model, how do you know if the model has skill or not?\n",
    "The  answer is to compare the results of a given classifier model to a baseline or naive classifier model.\n",
    "\n",
    "Consider a simple two-class classification problem where the number of observations is not\n",
    "equal for each class (e.g. it is imbalanced) with 25 examples for class-0 and 75 examples for\n",
    "class-1. This problem can be used to consider different naive classifier models. For example,\n",
    "consider a model that randomly predicts class-0 or class-1 with equal probability. How would it\n",
    "perform? We can calculate the expected performance using a simple probability model.\n",
    "\n",
    "$P(\\hat y = y) =P(\\hat y = 0)\\times P(y = 0) + P(\\hat y = 1) \\times P(y = 1)$\n",
    "\n",
    "\n",
    "We can plug in the occurrence of each class (0.25 and 0.75) and the predicted probability for each class (0.5 and 0.5) and estimate the performance of the model.\n",
    "\n",
    "$P(\\hat y = y) = 0.5 \\times 0.25 + 0.5 \\times 0.75$\n",
    "\n",
    "$P(\\hat y = y) = 0.5$\n",
    "\n",
    "\n",
    "It turns out that this classifier is pretty poor. Now, what if we consider predicting the majority class (class 1) every time? Again, we can plug in the predicted probabilities (0.0 and 1.0) and estimate the performance of the model.\n",
    "\n",
    "$P(\\hat y = y) = 0.0 \\times 0.25 + 1.0 \\times 0.75$\n",
    "\n",
    "$P(\\hat y = y) = 0.75$\n",
    "\n",
    "It turns out that this simple change results in a better naive classification model, and is perhaps the best naive classifier to use when classes are imbalanced.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html?highlight=dummyclassifier#sklearn.dummy.DummyClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.750 \n"
     ]
    }
   ],
   "source": [
    "# example of the majority class naive classifier in scikit-learn\n",
    "from numpy import asarray\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# define dataset\n",
    "X = asarray([0 for _ in range(100)])\n",
    "class0 = [0 for _ in range(25)]\n",
    "class1 = [1 for _ in range(75)]\n",
    "y = asarray(class0 + class1)\n",
    "\n",
    "# reshape data for sklearn\n",
    "X = X.reshape((len(X), 1))\n",
    "# define model\n",
    "model = DummyClassifier(strategy= 'most_frequent' )\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# make predictions\n",
    "yhat = model.predict(X)\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y, yhat)\n",
    "print( ' Accuracy: %.3f ' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task\n",
    "you must list two methods for calculating the effect size in applied machine  learning and when they might be useful. As a hint, consider one for the relationship between  variables and one for the difference between samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Scores #\n",
    "\n",
    "Predicting probabilities instead of class labels for a classification problem can provide additional nuance and uncertainty for the predictions. The added nuance allows more sophisticated metrics to be used to interpret\n",
    "and evaluate the predicted probabilities. Let’s take a closer look at the two popular scoring methods for evaluating predicted probabilities.\n",
    "\n",
    "## Log Loss Score ## \n",
    "\n",
    "Logistic loss, or log loss for short, calculates the log likelihood between the predicted probabilities\n",
    "and the observed probabilities. Although developed for training binary classification models like\n",
    "logistic regression, it can be used to evaluate multiclass problems and is functionally equivalent\n",
    "to calculating the cross-entropy derived from information theory. A model with perfect skill\n",
    "has a log loss score of 0.0. The log loss can be implemented in Python using the log loss()\n",
    "function in scikit-learn. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24691989080446483\n"
     ]
    }
   ],
   "source": [
    "# example of log loss\n",
    "from numpy import asarray\n",
    "from sklearn.metrics import log_loss\n",
    "# define data\n",
    "y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
    "# define data as expected, e.g. probability for each event {0, 1}\n",
    "y_true = asarray([[v, 1-v] for v in y_true])\n",
    "y_pred = asarray([[v, 1-v] for v in y_pred])\n",
    "# calculate log loss\n",
    "loss = log_loss(y_true, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8, 0.2],\n",
       "       [0.9, 0.1],\n",
       "       [0.9, 0.1],\n",
       "       [0.6, 0.4],\n",
       "       [0.8, 0.2],\n",
       "       [0.1, 0.9],\n",
       "       [0.4, 0.6],\n",
       "       [0.2, 0.8],\n",
       "       [0.1, 0.9],\n",
       "       [0.3, 0.7]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier Score##\n",
    "The Brier score, named for Glenn Brier, calculates the mean squared error between predicted probabilities and the expected values. The score summarizes the magnitude of the error in the probability forecasts. The error score is always between 0.0 and 1.0, where a model with perfect skill has a score of 0.0. The Brier score can be calculated in Python using the brier_score_loss() function in scikit-learn. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05700000000000001\n"
     ]
    }
   ],
   "source": [
    "# example of brier loss\n",
    "from sklearn.metrics import brier_score_loss\n",
    "# define data\n",
    "y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
    "# calculate brier score\n",
    "score = brier_score_loss(y_true, y_pred, pos_label=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must run each example and report the results, explain the meaning with your own words. As a bonus, change the mock predictions to make them better or worse and compare the resulting scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error types #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type I/II errors ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In hypothesis testing, significance level (often denoted as Greek letter alpha) is the probability of rejecting the null hypothesis (H0), when it was in fact true. A metric closely related to the significance level is the p-value, which is the probability of obtaining a result at least as extreme (a result even further from the null hypothesis), provided that the H0 was true. What does that mean in practice? In case of drawing a random sample from a population, it is always possible that the observed effect would have occurred only due to sampling error.\n",
    "\n",
    "When we reject a true H0 we are talking about a Type I error (false positive). This is the error connected to the significance level (see above). The other case occurs when we fail to reject a false H0, which is considered to be a Type II error (false negative).\n",
    "\n",
    "<img src=\"img/type_error.jpeg\" width=\"600\">\n",
    "\n",
    "\n",
    "### Confusion Matrix:### \n",
    "A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/Confusion_Matrix1_1.png\" width=\"350\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Definition of the Terms:\n",
    "\n",
    "    Positive (P) : Observation is positive (for example: is an apple).\n",
    "    Negative (N) : Observation is not positive (for example: is not an apple).\n",
    "    True Positive (TP) : Observation is positive, and is predicted to be positive.\n",
    "    False Negative (FN) : Observation is positive, but is predicted negative.\n",
    "    True Negative (TN) : Observation is negative, and is predicted to be negative.\n",
    "    False Positive (FP) : Observation is negative, but is predicted positive.\n",
    "\n",
    "\n",
    "### Classification Rate/Accuracy:\n",
    "Classification Rate or Accuracy is given by the relation:\n",
    "\n",
    "$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "However, there are problems with accuracy. It assumes equal costs for both kinds of errors. A 99% accuracy can be excellent, good, mediocre, poor or terrible depending upon the problem.\n",
    "\n",
    "### Recall:\n",
    "\n",
    "$Recall = \\frac{TP}{TP+FN} $\n",
    "\n",
    "Recall can be defined as the ratio of the total number of correctly classified positive examples divide to the total number of positive examples. High Recall indicates the class is correctly recognized (a small number of FN).\n",
    "\n",
    "### Precision:\n",
    "$Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "To get the value of precision we divide the total number of correctly classified positive examples by the total number of predicted positive examples. High Precision indicates an example labelled as positive is indeed positive (a small number of FP).\n",
    "\n",
    "#### High recall, low precision: This means that most of the positive examples are correctly recognized (low FN) but there are a lot of false positives.\n",
    "\n",
    "#### Low recall, high precision: This shows that we miss a lot of positive examples (high FN) but those we predict as positive are indeed positive (low FP) \n",
    "\n",
    "## Example\n",
    "\n",
    "If a classification system has been trained to distinguish between cats and dogs, a confusion matrix will summarize the results of testing the algorithm for further inspection. Assuming a sample of 13 animals — 8 cats and 5 dogs — the resulting confusion matrix could look like the table below: \n",
    "\n",
    "<img src=\"img/matrix.png\" width=\"200\">\n",
    "\n",
    "In this confusion matrix, of the 8 actual cats, the system predicted that three were dogs, and of the five dogs, it predicted that two were cats. All correct predictions are located in the diagonal of the table (highlighted in bold), so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3],\n",
       "       [2, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y = np.array([0,0,0,0,0,0,0,0,1,1,1,1,1,1,1])\n",
    "Yp= np.array([1,0,1,0,1,0,0,0,1,0,1,1,1,1,0])\n",
    "\n",
    "confusion_matrix(Y, Yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition a confusion matrix $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$\n",
    "\n",
    "Thus in binary classification, the count of true negatives is  $C_{0,0}$, false negatives is  $C_{1,0}$, true positives is  $C_{1,1}$ and false positives is  $C_{0,1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     y\u001b[38;5;241m=\u001b[39mdata[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x,y\n\u001b[1;32m---> 17\u001b[0m X,Y\u001b[38;5;241m=\u001b[39m\u001b[43mloadDiab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y\u001b[38;5;241m.\u001b[39mshape)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mloadDiab\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadDiab\u001b[39m():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#define el nombre del archivo y las etiquetas para cada tipo de flor\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_lb1/diabetes.data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mloadtxt(file_name,delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     11\u001b[0m     x\u001b[38;5;241m=\u001b[39mdata[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     12\u001b[0m     y\u001b[38;5;241m=\u001b[39mdata[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#diabetes\n",
    "#Class Value  Number of instances\n",
    "#   0            500\n",
    "#   1            268\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "def loadDiab():\n",
    "    #define el nombre del archivo y las etiquetas para cada tipo de flor\n",
    "    file_name='data_lb1/diabetes.data'\n",
    "    data=np.loadtxt(file_name,delimiter=',') \n",
    "    x=data[:,:-1]\n",
    "    y=data[:,-1]\n",
    "\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "X,Y=loadDiab()\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "(537, 8)\n",
      "(537,)\n",
      "Test set\n",
      "(231, 8)\n",
      "(231,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "print('Training set')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Test set')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120,  31],\n",
       "       [ 30,  50]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42, multi_class='ovr')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "Yp = clf.predict(X_test)\n",
    "confusion_matrix(y_test, Yp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
