{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomendación: \n",
    "\n",
    "Leer atentamente todo el documento, analizar cada linea de codigo, presentar las soluciones en cada item donde dice \"Su Tarea\".\n",
    "\n",
    "Al finalizar, debe exportar este archivo, con las modificaciones que haya hecho, a un archivo html o pdf, y es ese archivo el que debe enviar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio ejemplo guiado - Aprendizaje de Maquinas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchos servicios de correo proveen un filtro para diferenciar entre correos spam y no-spam con alta precisión. En esta parte del ejercicio usted construirá su propio filtro de spam. Debe entrenar un sistema para clasificar si un correo dado $x$, es spam $(y=1)$ o no-spam $(y=0)$. En particular, se requiere convertir cada correo en un vector de características $x ∈ R^n$. En las siguientes partes del ejercicio se le indicaran los pasos que debe seguir para construir tal vector de características a partir de un correo. \n",
    "Para este ejercicio usted usara la base de datos publica SpamAssasin, y se empleara solo el cuerpo del correo, excluyendo la cabecera.\n",
    "Antes de iniciar cualquier proceso de aprendizaje automático, es bueno mirar un ejemplo del conjunto de correos. Por ejemplo la siguiente figura muestra el contenido de un correo que contiene URL, una dirección correo electrónico, números y cantidades de dinero en dolares.\n",
    "\n",
    "<img src=\"eximg/img1.png\" width=\"600\">\n",
    "\n",
    "\n",
    "Aunque muchos correos contienen información similar, nunca se presenta con el mismo formato. Por lo tanto, un método que se usa a menudo es normalizar estos valores de tal forma que todas las URLs se tratan de la misma forma, todos los números igual, etc. Por ejemplo, se puede reemplazar cualquier URL por un string único \"httpaddr\", para indicar que en esa parte hay una URL. Hacer esto permite que el clasificador tome la decisión basado en si hay cualquier URL en el correo, en lugar de una URL especifica. \n",
    "\n",
    "\n",
    "Antes de continuar es necesario instalar la siguiente librería en su entorno de anaconda. Esto permitirá facilitar el proceso de normalizar los correos electrónicos. \n",
    "\n",
    "\"Natural Language Toolkit\": Una librería de procesamiento de lenguaje natural \"Natural Language Tool Kit - NTLK\". Documentación extendida se puede encontrar en https://www.nltk.org/\n",
    "\n",
    "La instalacion se puede usar uno de los siguientes metodos (intentar primero con conda) desde la ventana de comandos de Anaconda Promt o en linux desde la terminal.\n",
    "\n",
    "``` python\n",
    "conda install -c anaconda nltk \n",
    "\n",
    "pip install nltk\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizar correos electronicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lleva a cabo el siguiente proceso, no necesariamente en el orden descrito a continuación. Su tarea es analizar el código fuente e identificar donde se lleva a cabo cada paso:\n",
    "1. pasar a minúsculas: Todo el email es convertido a minúsculas, de tal forma que \"IndIcaTE\" se trata igual que \"indicate\").\n",
    "2. Normalizar todas las URLs: donde haya una URL se reemplaza por el texto  “httpaddr”.\n",
    "3. Normalizar direcciones de correo: Cada que haya una dirección de correo se reemplaza por el texto  “emailaddr”.\n",
    "4. Normalizar números: Cada vez que haya un numero o secuencia de dígitos, se reemplaza por la palabra “number”.\n",
    "5. Normalizar el signo $ \\$ $: en este caso particular se reemplaza el signo $ \\$ $ por la palabra “dollar”, pues los correos son en Inglés.\n",
    "6. Reducir a raíz: Las palabras son reemplazadas por su raiz, por ejemplo: “discount”, “discounts”, “discounted” y “discounting” todas se reemplazan por  “discount”.\n",
    "\n",
    "Para este proceso se usa el algoritmo propuesto en el siguiente articulo:\n",
    "Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14,  no. 3, pp 130-137\n",
    "\n",
    "ver ejemplo en \n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "\n",
    "<img src=\"eximg/img2.png\" width=\"400\">\n",
    "\n",
    "7. Un proceso necesario pero que no se realiza en este ejemplo es eliminar aquellas secuencias que no son palabras por ejemplo “UBJjjsPPj” o “vvvsssdd” no son palabras y deberían eliminarse. Tampoco se han eliminado tags de HTML, algo común en los correos electrónicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es definir el codigo en python que nos permite realizar el proceso descrito anteriormente. En las siguientes celdas se tienen definidas algunas funciones. \n",
    "\n",
    "1. processEmail(): que recibe un nombre de un archivo y retorna una lista de palabras ya procesadas. ............. \n",
    "2. stemSentence(): ....\n",
    "\n",
    "Consultar que son expresiones regulares\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "#esta funcion permite:\n",
    "#    \n",
    "# \n",
    "#\n",
    "def stemSentence(sentence): \n",
    "    porter = PorterStemmer()\n",
    "    # ?\n",
    "    print(sentence)\n",
    "    token_words=word_tokenize(sentence) \n",
    "    print(token_words)\n",
    "    stem_sentence=[] \n",
    "    for word in token_words:\n",
    "        #?\n",
    "        if len(word)>1:\n",
    "            #?\n",
    "            word=word.lower()            \n",
    "            stem_sentence.append(porter.stem(word)) \n",
    "             \n",
    "    return stem_sentence\n",
    "########################################################################################\n",
    "def processEmail(file_name):\n",
    "    # ?\n",
    "    print('Procesando: ', file_name)\n",
    "    hf = open(file_name,'r')\n",
    "    lines=hf.read() \n",
    "    hf.close()\n",
    "    # ?\n",
    "    lines=lines.replace('\\n',' ')\n",
    "    lines=lines.replace('\\r',' ')\n",
    "    lines=lines.replace('\\t',' ')\n",
    "    lines=lines.replace('-',' ')    \n",
    "    \n",
    "    #?\n",
    "    lines=re.sub(r'[0-9]+','number',lines)\n",
    "    # ?\n",
    "    lines=lines.replace('$','dollar')\n",
    "    # ?\n",
    "    lines = re.sub('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+','httpaddr',lines)\n",
    "    # ?\n",
    "    lines =  re.sub('[^\\s]+@[^\\s]+','emailaddr', lines) \n",
    "    # ?\n",
    "    lines=re.sub(r'[@/#.:&*+=\\[\\]?!(){},\\'\\'\">_<;%]+','',lines)\n",
    "    # ?\n",
    "    lines = ' '.join(lines.split())\n",
    "    \n",
    "    #\n",
    "    # usar la funcion stemSentence()\n",
    "    lines = stemSentence(lines)\n",
    "    \n",
    "    print('Proceso finalizado....')\n",
    "    return lines\n",
    "########################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Su tarea:\n",
    "realizar un analisis de las funciones que se implementan en la celda anterior y redactar su explicación aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemSentence(sentence)................\n",
    "\n",
    "\n",
    "\n",
    "processEmail(file_name)..............\n",
    "\n",
    "\n",
    "#presionar ctrl enter .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacer uso de las funciones descritas anteriormente\n",
    "#preprocesar correo electronico\n",
    "#Ver por ejemplo cual es el resultado de procesar los archivos \n",
    "#emailSample1.txt, emailSample2.txt\n",
    "#spamSample1.txt y spamSample2.txt \n",
    "#que están en la carpeta exam2Data. \n",
    "\n",
    "#\n",
    "\n",
    "file_name='exData/spamSample1.txt'\n",
    "palabras = processEmail(file_name)\n",
    "print('Resultado:\\n')\n",
    "print(' '.join(palabras))\n",
    "#investigar que hace la funcion  ' '.join() usada en la linea anterior\n",
    "\n",
    "#\n",
    "#guardar el resultado en un archivo de texto\n",
    "#verificar con un editor de texto que se haya guardado correctamente\n",
    "#el archivo resultado1.txt\n",
    "\n",
    "np.savetxt('resultado1.txt', [' '.join(palabras)], delimiter=\" \", fmt=\"%s\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Su tarea:\n",
    "Copiar y pegar el contenido de la celda anterior en otra celda para ilustar los resultados con otros correos de  ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario\n",
    "Después del preproceso anterior, el resultado es una lista de palabras para cada correo electrónico. El siguiente paso es seleccionar cual de esas palabras nos gustaría usar en nuestro clasificador y cuales debemos dejar por fuera. \n",
    "\n",
    "Para este ejercicio se han seleccionado solo una lista de las palabras mas frecuentes, y se le ha llamado }\n",
    "\"Vocabulario\". Dado que hay palabras que ocurren de forma muy esporádica en ciertos correos, estas pueden causar valores atípicos y que nuestro modelo quede sobre entrenado o tenga problemas de convergencia, por lo tanto no se tendrán en cuenta.  La lista completa de palabras la encuentra en el archivo vocab.txt, en la carpeta exam2Data. Ésta lista fue generada al seleccionar las palabras que ocurren por lo menos 100 veces en la base de datos total, lo que resulta en una lista de 1.899 palabras. En aplicaciones prácticas un vocabulario puede contar con 10.000 o 50.000 palabras. \n",
    "\n",
    "Hay varias formas de convertir datos de texto a datos numéricos.  Por ejemplo, dado el vocabulario, es posible mapear cada palabra en el correo electrónico preprocesado en una lista que contiene un índice. Este índice o numero representa la palabra del vocabulario que aparece en el correo. Ver el siguiente ejemplo para mayor claridad.\n",
    "\n",
    "<img src=\"exam2img/img3.png\" width=\"600\">\n",
    "\n",
    "En la parte superior se tiene el correo electrónico preprocesado, en la parte inferior izquierda se tiene una pequeña muestra del vocabulario con índices asociados, es decir, cada palabra se le asocia un numero en orden de aparición (iniciando en 1 y terminando en 1899). Al lado inferior derecho tenemos el correo mapeado con índices, donde cada palabra se ha reemplazado por el índice o número correspondiente según como aparece en el vocabulario.   Por ejemplo, la palabra anyon esta en la posición 86 en el vocabulario, entonces se usa el numero 86 para mapear la primer palabra del correo. Igual se hace con know, que corresponde a la palabra 916, entonces “anyon know” se mapea como [86 916]. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se define una funcion que permite leer el archivo de texto donde se encuentra el vocabulario\n",
    "#######################################################################################\n",
    "def leerVocab():\n",
    "    #leer vocabulario\n",
    "    index=[]\n",
    "    word=[]\n",
    "    hf = open('exam2Data/vocab.txt','r')\n",
    "    vocab=hf.readlines() \n",
    "    hf.close()\n",
    "    for line in vocab:\n",
    "        i,w=line.strip('\\n').split('\\t')\n",
    "        index.append(int(i))\n",
    "        word.append(w)\n",
    "    #retornar lista de indices y palabras    \n",
    "    return index,word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Su tarea \n",
    "\n",
    "Implementar el código necesario para realizar este mapeo, es decir, convertir una lista de palabras del correo preprocesado a una lista de índices. Esta lista de indices la puede guardar en la variable mapeo_indices que ya esta inicializada en el codigo a continuación.\n",
    "\n",
    "En este ejercicio hace uso de la función leerVocab(), que hace el trabajo de leer el archivo y retornar dos listas, una con los índices y otra con las palabras del vocabulario. \n",
    "\n",
    "index,vocab=leerVocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando:  exam2Data/spamSample2.txt\n",
      "Proceso finalizado....\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "file_name='exam2Data/spamSample1.txt'\n",
    "palabras = processEmail(file_name)\n",
    "\n",
    "#cargar lista del vocabulario\n",
    "index,vocab=leerVocab()\n",
    "\n",
    "mapeo_indices=[]\n",
    "#################\n",
    "#su codigo va aqui\n",
    "\n",
    "###\n",
    "\n",
    "print(mapeo_indices)\n",
    "############################\n",
    "#guardar el resultado, este archivo es el que usted debe subir, verificar su contenido antes de subirlo\n",
    "np.savetxt('resultado1.txt', mapeo_indices, delimiter=\" \", fmt=\"%s\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer características de los correos electrónicos\n",
    "\n",
    "Su trabajo es implementar la etapa para extraer características, este proceso permite convertir cada correo electrónico en un vector $x ∈ R^n$. Para este ejercicio, usted usara $n$ = número de palabras en el vocabulario, es decir $n=1899$. Específicamente, la característica puede tomar solo dos valores $x_i ∈ {0, 1}$ para un correo corresponde a determinar si la $i$-ésima palabra del vocabulario está también en el correo. Es decir, $x_i=1$ si la $i$-esima palabra SI esta en el correo, $x_i=0$ si la i-ésima palabra NO está presente en el correo. Un vector de características para un correo electrónico se debería ver como se muestra en la siguiente figura. \n",
    "\n",
    "<img src=\"exam2img/img4.png\" width=\"100\">\n",
    "\n",
    "## Tip:## \n",
    "\n",
    "Usted puede usar la funcionalidad intersección de numpy. Se aplica sobre dos conjuntos representados en arrays o listas, y calcula la intersección entre los dos conjuntos:\n",
    "Considere las listas \n",
    "\n",
    "```python\n",
    "x=['a','b','c','d','e','f','g','h','i']                                                                       \n",
    "y=['a','a','c','f','i']                                                                                   \n",
    "c1,c2,c3=np.intersect1d(x,y,return_indices=True)\n",
    "```\n",
    "En c1 retorna los elementos comunes entre las dos listas, en c2 un array de índices de la primer lista que también se encuentran en la segunda, y en c3 un array de índices de elementos de la segunda lista que también están en la primer lista. Verificar el resultado, usted puede extender la lista x, o la lista y, observar el resultado.\n",
    "\n",
    "También puede usar la instrucción in que sirve para preguntar si un elemento está en una lista o arreglo:\n",
    "\n",
    "```python\n",
    "'a' in x\n",
    "True\n",
    "\n",
    "'z' in x\n",
    "False\n",
    "```\n",
    "\n",
    "\n",
    "## Su tarea\n",
    "Implementar el codigo que permita generar un vector de caracteristicas para cada correo. A continuación se presenta el código base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando:  exam2Data/spamSample1.txt\n",
      "Proceso finalizado....\n"
     ]
    }
   ],
   "source": [
    "#cargar un correo y procesarlo\n",
    "file_name='exam2Data/spamSample1.txt'\n",
    "palabras = processEmail(file_name)\n",
    "\n",
    "#cargar lista del vocabulario\n",
    "index,vocab=leerVocab()\n",
    "\n",
    "#generar un vector de 1 x 1899 y guardarlo en un archivo de texto, puede usar la variable vector\n",
    "#guardar el resultado\n",
    "#verificar su contenido antes de subirlo\n",
    "vector = np.zeros(1899) #vector con solo ceros\n",
    "### su codigo va aqui\n",
    "\n",
    "\n",
    "###\n",
    "np.savetxt('resultado1.txt', vector, delimiter=\" \", fmt=\"%s\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar su sistema de clasificación de spam\n",
    "Una vez usted haya completado la etapa de extraer características, el siguiente paso es cargar la base de datos que ya ha sido preprocesada, usando el mismo algoritmo descrito previamente, y a la cual ya se le han extraído las características. Los datos se encuentran en los archivos <b>spamTrain.data </b > que contiene 4000 ejemplos de correo spam y no spam, y <b>spamTest.data</b> que contiene 1000 ejemplos para evaluar su sistema. Cada ejemplo en los conjuntos anteriores corresponde a un vector de 1899 características. Los archivos contienen una columna adicional con la etiqueta del ejemplo.\n",
    "\n",
    "## Su tarea\n",
    "\n",
    "Implementar el codigo en python para cargar los archivos de entrenamiento y prueba. Separar los datos las variables: en X_train, y_train, donde X_train corresponde a las primeras 1899 columnas de los datos en el archivo y y_train corresponde a la ultima. Tambien generar X_test, y_test a partir del archivo de prueba que tiene una estructura similar.\n",
    "\n",
    "El resultado son 4 variables\n",
    "1. X_train, y_train : corresponde a los datos de entrenamiento\n",
    "2. X_test, y_test : corresponde a los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "## Su tarea\n",
    "\n",
    "1. Consultar que significan las siguientes medidas: $accuracy$, $recall$ y $precision$. Consultar que es la matriz de confusión y que mide en un sistema de clasificación. Que son Verdaderos Negativos, Verdaderos Positivos, Falsos Negativos y Falsos Positivos?\n",
    "\n",
    "2. Enrenar por lo menos dos clasificadores diferentes: Seleccionar por ejemplo un KNN, clasificador usando regresión logistica o maquinas de soporte vectorial, y debe realizar el proceso de entrenamiento ajustando los parámetros de tal forma que el desempeño de cada clasificador sea lo mejor posible.\n",
    "\n",
    "### Tip:\n",
    "Pueden usar: $accuracy$, $recall$ y $precision$ \n",
    "\n",
    "Para calcular la matriz de confusion:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  #etiquetas verdaderas\n",
    "y_pred = [0, 0, 1, 1, 0, 1, 1, 1, 1, 0]  #etiquetas que predice el clasificador\n",
    "cm = confusion_matrix(y_true, y_pred) \n",
    "print(cm)\n",
    "```\n",
    "\n",
    "Tambien pueden usar la función  \"classification_report\":\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 0, 1, 1, 0, 1, 1, 1, 1, 0]\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "```\n",
    "Ver la documentacón : \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "\n",
    "\n",
    "Una vez entrenando el modelo, usted puede guardarlo y cargarlo de nuevo, no es necesario entrenar cada vez que quiera evaluarlo, pues el proceso de ajuste de parámetros toma tiempo. \n",
    "\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "#clasificador\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf =  LogisticRegression()\n",
    "\n",
    "#Codigo para cargar los datos\n",
    "#\n",
    "#\n",
    "\n",
    "#Codigo para entrenar el clasificador usando X_train, y_train\n",
    "#\n",
    "#\n",
    "\n",
    "#Codigo para evaluar el clasificador usando X_test, y_test\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponga que usted tiene un clasificador ya entrenado y lo tiene en la variable clf, entonces puede guardarlo en un archivo binario y cargarlo de nuevo de la siguiente forma:\n",
    "\n",
    "```python\n",
    "#guardar el clasificador clf\n",
    "pickle.dump(clf, open( \"mi_clasficador.p\", \"wb\" ) )\n",
    "\n",
    "#borrar la variable \n",
    "del clf #el clasificador ya no existe\n",
    "\n",
    "#cargarlo de nuevo\n",
    "clf = pickle.load( open( \"mi_clasficador.p\", \"rb\" ) )\n",
    "\n",
    "#puede volver a utilizarlo\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar los correos de ejemplo\n",
    "Una vez tenga su sistema entrenado, usted puede probar si funciona y puede clasificar correctamente un correo electrónico. En la carpeta exam2Data hay 4 ejemplos:  emailSample1.txt, emailSample2.txt, spamSample1.txt y spamSample2.txt\n",
    "\n",
    "Usted debe implementar el código necesario para evaluarlos. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Su Tarea\n",
    "Guardar en archivos de texto algunos de sus propios correos que tenga en idioma inglés, tratar que no incluya tags HTML, únicamente texto plano, y probar su sistema de clasificación. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluar su clasificador de forma individual con correos ejemplo\n",
    "### su codigo va aqui \n",
    "\n",
    "\n",
    "### ver el ejemplo a continuacion para evaluar un solo ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#suponga que usted tiene un clasficador que ha sido entrenado sobre 100 datos, y\n",
    "#con 30 variables:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "X = np.random.randn(100,30)              #datos\n",
    "y = (2*np.random.rand(100)).astype(int) #vector aleatorio de 0 y 1\n",
    "X.shape\n",
    "clf.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#Suponga que quiere evaluarlo con un solo patron, es decir un solo vector de caracteristicas\n",
    "x_test = np.random.randn(30)\n",
    "\n",
    "#evaluar para tener una predicción\n",
    "#es necesario aplicar reshape para que se entienda que es un solo vector \n",
    "#de 30 caracteristicas y no 30 ejemplos de una sola caracteristica\n",
    "\n",
    "c=clf.predict(x_test.reshape(1, -1))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
